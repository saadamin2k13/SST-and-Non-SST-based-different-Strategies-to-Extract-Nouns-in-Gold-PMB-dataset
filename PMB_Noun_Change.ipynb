{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "lwOuhI9PltdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8QfJbT1jryt"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train10000.txt.raw') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "SLYR8rdllyBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eztBfiNAm2Qp",
        "outputId": "0fbe348d-46ea-46bb-9277-1830082069d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg "
      ],
      "metadata": {
        "id": "zgAgE48FnTo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "SJbhu5pJm7SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=open('train10000.txt.raw').read()"
      ],
      "metadata": {
        "id": "sZibeSYioY7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU3AZcRNB9V5",
        "outputId": "ab0015ba-0c90-4613-d120-4b73fe836db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "_ad_ZZcToIjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting all Nouns from Text**"
      ],
      "metadata": {
        "id": "6HSQsr7NnOkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "text=open('test.txt.raw').read()\n",
        "doc = nlp(text)\n",
        "\n",
        "#doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "with open(r'all-nouns.txt','w') as n:\n",
        "\n",
        "  for chunk in doc.noun_chunks:\n",
        "    n.write(\"%s\\n\" % chunk.text)\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "id": "EisK0fssaK5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing named-entites form text file**"
      ],
      "metadata": {
        "id": "GB8BAi_e5Rix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "text_data = open('all-nouns.txt').read()\n",
        "#text_data = 'New York is in USA'\n",
        "document = nlp(text_data)\n",
        "with open(r'without-named-entities.txt', 'w') as fp:\n",
        "  fp.write((\" \".join([ent.text for ent in document if not ent.ent_type_])))\n",
        "  print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9WxaB6c5U8D",
        "outputId": "c73ca143-5ea1-4b15-dd5b-d419cd7729b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing empty spaces from data**"
      ],
      "metadata": {
        "id": "BVYCqAubJ9y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"without-named-entities.txt\",\"r\") as f, open(\"wo-named-entities.txt\",\"w\") as outfile:\n",
        " for i in f.readlines():\n",
        "       if not i.strip():\n",
        "           continue\n",
        "       if i:\n",
        "           outfile.write(i) "
      ],
      "metadata": {
        "id": "sZ_Ub7XQKCB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing stop-words**"
      ],
      "metadata": {
        "id": "yQbSQel1F5wH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using SpaCy\n",
        "**we get better results with SpaCy. so, we will continue with SpaCy model.**"
      ],
      "metadata": {
        "id": "q_BuQdmaRmwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en\n"
      ],
      "metadata": {
        "id": "ju3-O3vORomw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = open('wo-named-entities.txt').read()\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "output = open('without-stopwords.txt','w')\n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word)\n",
        "#print(token_list)\n",
        "#output.write(str(filtered_sentence))\n",
        "#print(filtered_sentence)   \n",
        "\n",
        "my_clean_txt = \" \".join(filtered_sentence)\n",
        "output.write(my_clean_txt)\n",
        "print(my_clean_txt)"
      ],
      "metadata": {
        "id": "8EdzDllgR5fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing empty lines form data**"
      ],
      "metadata": {
        "id": "6vTUivNcLEkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"nouns.txt\",\"r\") as f, open(\"wo-whitespace.txt\",\"w\") as outfile:\n",
        " for i in f.readlines():\n",
        "       if not i.strip():\n",
        "           continue\n",
        "       if i:\n",
        "           outfile.write(i) "
      ],
      "metadata": {
        "id": "MZeqiRmXLIBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first get all lines from file\n",
        "with open('wo-whitespace.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# remove spaces\n",
        "lines = [line.replace(' ', '') for line in lines]\n",
        "\n",
        "# finally, write lines in the file\n",
        "with open('wo-whitespace.txt', 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "metadata": {
        "id": "9GO3zvKBsE2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**getting duplicate values**"
      ],
      "metadata": {
        "id": "5L7GQHOZIg0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst = []\n",
        "with open (\"wo-whitespace.txt\") as f, open (\"duplicate-words.txt\", \"w\") as f2:\n",
        "    for line in f:\n",
        "        for word in line.split(): #splited by gaps (space)\n",
        "            if word not in lst:\n",
        "                lst.append(word)\n",
        "                #f2.write(str(lst.append(word)))\n",
        "            else:\n",
        "                #print (word)\n",
        "                f2.write(\"%s\\n\" % word)"
      ],
      "metadata": {
        "id": "E1PFXU_5s1DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extracting back the sentances that belong to the given nouns**"
      ],
      "metadata": {
        "id": "sV0pW1drK24H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting complete sentence from Nouns inside the sentence.**\n",
        "this is to map nouns and actual sentence..."
      ],
      "metadata": {
        "id": "0biy5bc-C9gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code below will create saperate files for each noun-name. that file contain all sentences with the noun provided."
      ],
      "metadata": {
        "id": "vXb6V1UP8V_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read the file containing the list of words\n",
        "with open('word_list.txt', 'r') as f:\n",
        "    word_list = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Create a dictionary to store sentences by word\n",
        "sentences_by_word = {word: [] for word in word_list}\n",
        "\n",
        "# Open and read the file containing sentences\n",
        "with open('sentences.txt', 'r') as f:\n",
        "    sentences = f.readlines()\n",
        "\n",
        "# Loop through each sentence and check if it contains any of the words\n",
        "for sentence in sentences:\n",
        "    for word in word_list:\n",
        "        if word in sentence:\n",
        "            sentences_by_word[word].append(sentence.strip())\n",
        "\n",
        "# Print out the sentences for each word and save them to a file\n",
        "for word in word_list:\n",
        "    print(f\"Sentences containing '{word}':\")\n",
        "    with open(f\"{word}_sentences.txt\", \"w\") as f:\n",
        "        for sentence in sentences_by_word[word]:\n",
        "            print(sentence)\n",
        "            f.write(sentence + \"\\n\")\n",
        "    print(f\"Sentences containing '{word}' have been saved to '{word}_sentences.txt'.\\n\")\n"
      ],
      "metadata": {
        "id": "MAO2BFJNtWpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_original = open('train.txt').read()\n",
        "textual_mapping = open('textual-mapping.txt', 'w')\n",
        "# lines_1 = []\n",
        "# with open(\"dev.txt.raw\") as file:\n",
        "#     for line in file: \n",
        "#         line = line.strip() #or some other preprocessing\n",
        "#         lines_1.append(line) #storing everything in memory!\n",
        "\n",
        "\n",
        "# Here i am reading replaceable names in the dataset.\n",
        "lines_2 = []\n",
        "with open(\"duplicate-words.txt\") as file:\n",
        "    for line in file: \n",
        "        line = line.strip() #or some other preprocessing\n",
        "        lines_2.append(line) #storing everything in memory!\n",
        "\n",
        "\n",
        "\n",
        "#text = 'I like blueberry icecream. He has a green car. She has blue car. I love dogs.'\n",
        "#with open('')\n",
        "for i in lines_2:\n",
        "  #print([x for x in text_original.split('.') if any(y in x for y in [i])])\n",
        "  textual_mapping.write(str([ x for x in text_original.split('\\n') if any(y in x for y in [ i ])])+'\\n')"
      ],
      "metadata": {
        "id": "jIiDuImMDJcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_original = open('dev.txt.raw').read()\n",
        "textual_mapping = open('textual-mapping2.txt', 'w')\n",
        "# lines_1 = []\n",
        "# with open(\"dev.txt.raw\") as file:\n",
        "#     for line in file: \n",
        "#         line = line.strip() #or some other preprocessing\n",
        "#         lines_1.append(line) #storing everything in memory!\n",
        "\n",
        "\n",
        "# Here i am reading replaceable names in the dataset.\n",
        "lines_2 = []\n",
        "with open(\"all-nouns2.txt\") as file:\n",
        "    for line in file: \n",
        "        line = line.strip() #or some other preprocessing\n",
        "        lines_2.append(line) #storing everything in memory!\n",
        "\n",
        "\n",
        "\n",
        "#text = 'I like blueberry icecream. He has a green car. She has blue car. I love dogs.'\n",
        "#with open('')\n",
        "for i in lines_2:\n",
        "  #print([x for x in text_original.split('.') if any(y in x for y in [i])])\n",
        "  textual_mapping.write(str([x for x in text_original.split('\\n') if any(y in x for y in [ i ])])+'\\n')"
      ],
      "metadata": {
        "id": "UijIQXBhQB5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat all-nouns1.txt all-nouns2.txt > all-nouns.txt\n",
        "!cat textual-mapping1.txt textual-mapping2.txt > textual-mapping.txt"
      ],
      "metadata": {
        "id": "ye1gTgVDtFc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = open('textual-mapping1.txt').read()\n",
        "# #print(type(input))\n",
        "# # output = input.split(',')\n",
        "# # print(output, '\\n')\n",
        "\n",
        "# text_file = open(r'textual-mapping-output.txt', 'w')\n",
        "# my_list = input.split(\"', \")\n",
        "\n",
        "# for line in my_list:\n",
        "#     #text_file.write(line + '\\n')\n",
        "#     text_file.write(line)\n",
        "\n",
        "# text_file.close()"
      ],
      "metadata": {
        "id": "haGorA--guiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**removing duplication in nouns file**"
      ],
      "metadata": {
        "id": "ikotK-DlASa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sort -u wo-stopwords.txt > unique-nouns.txt\n"
      ],
      "metadata": {
        "id": "eMID_KL3Bu_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Annotation with supersense tagging (SST)**\n",
        "https://github.com/booknlp/booknlp"
      ],
      "metadata": {
        "id": "PtsGN3gcQhia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install booknlp\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "TSLT9cXUQks5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from booknlp.booknlp import BookNLP\n",
        "\n",
        "# model_params={\n",
        "# \t\t\"pipeline\":\"entity,quote,supersense,event,coref\", \n",
        "# \t\t\"model\":\"big\"\n",
        "# \t}\n",
        "\n",
        "######## i am only interested in SuperSenses model parameters so i am using only SuperSense.\n",
        "model_params={\n",
        "\t\t\"pipeline\":\"entity,supersense\", \n",
        "\t\t\"model\":\"big\"\n",
        "\t}\n",
        "\n",
        "\n",
        "booknlp=BookNLP(\"en\", model_params)\n"
      ],
      "metadata": {
        "id": "4cisIKP1Q3wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb56bf0-8f1e-4528-bff0-2afb462c6c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pipeline': 'entity,supersense', 'model': 'big'}\n",
            "--- startup: 3.275 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Input file to process\n",
        "input_file=\"CD_sentences.txt\"\n",
        "\n",
        "# Output directory to store resulting files in\n",
        "output_directory=\"output_dir/bartleby/\"\n",
        "\n",
        "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
        "book_id=\"bartleby\"\n",
        "\n",
        "booknlp.process(input_file, output_directory, book_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxE0phRgRQwj",
        "outputId": "cf5a8e57-d38c-4b2a-8e76-d0980c40fa29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- spacy: 0.021 seconds ---\n",
            "--- entities: 0.495 seconds ---\n",
            "--- quotes: 0.000 seconds ---\n",
            "--- name coref: 0.000 seconds ---\n",
            "--- TOTAL (excl. startup): 4.290 seconds ---, 62 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**code to read all text files and apply bertleby supersene and save each file independently.**"
      ],
      "metadata": {
        "id": "2ZBJ0RdtC-12"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMYoqBExDK4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**converting nouns into lower-case**"
      ],
      "metadata": {
        "id": "0iPhSfaZgfNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dd if=nouns.txt of=lower-case-nouns.txt conv=lcase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jET8RXxPgio_",
        "outputId": "e23338f2-d415-41e1-cf35-1cdb7c622cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15+1 records in\n",
            "15+1 records out\n",
            "7707 bytes (7.7 kB, 7.5 KiB) copied, 0.000321949 s, 23.9 MB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing duplication in nouns"
      ],
      "metadata": {
        "id": "YuHGCeB5c_y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sort -u lower-case-nouns.txt > lc-unique-nouns.txt"
      ],
      "metadata": {
        "id": "5lvYxJ30dCMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing white spaces"
      ],
      "metadata": {
        "id": "FS7QAAjWpzb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first get all lines from file\n",
        "with open('nouns.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# remove spaces\n",
        "lines = [line.replace(' ', '') for line in lines]\n",
        "\n",
        "# finally, write lines in the file\n",
        "with open('nouns.txt', 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "metadata": {
        "id": "s5tT88A-p1tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**generating dictionary for unlex model**"
      ],
      "metadata": {
        "id": "LKV3uJsMlo0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here i am reading original names of train.txt.raw dataset file.\n",
        "lines_1 = []\n",
        "with open(\"nouns.txt.raw\") as file:\n",
        "    for line in file: \n",
        "        line = line.strip() #or some other preprocessing\n",
        "        lines_1.append(line) #storing everything in memory!\n",
        "\n",
        "\n",
        "# Here i am reading replaceable names in the dataset.\n",
        "lines_2 = []\n",
        "with open(\"unlex.txt.raw\") as file:\n",
        "    for line in file: \n",
        "        line = line.strip() #or some other preprocessing\n",
        "        lines_2.append(line) #storing everything in memory!\n",
        "\n",
        "for x,y in zip(lines_1,lines_2): # <--- Loop through the list to check      \n",
        "  print(\"'\"+x+\"'\",\":\",\"'\"+y+\"'\",\",\")"
      ],
      "metadata": {
        "id": "Az4RguZ5luzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('nouns.txt', 'r') as infile, open('noun1.txt', 'w') as outfile:\n",
        "    data = infile.readlines()\n",
        "    for i in data:\n",
        "        outfile.write(str(i.split())+'\\n')"
      ],
      "metadata": {
        "id": "eB5NI8FAh8rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBDr2GO43k02",
        "outputId": "c19a56b1-9916-4b72-e762-318816848a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cap-case-nouns.txt  nouns.txt  test.txt.raw  unlex-nouns.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**getting word synonyms using BERT**\n",
        "https://github.com/cedricconol/bertosaurus\n",
        "\n",
        "\n",
        "\n",
        "1.   This implementation is done in PyCharm with project name \"**Bert-Synonyms**\"\n",
        "2. To run this implementation, have to follow the following steps\n",
        "3. activate conda environemnt using command [**conda activate synonyms**]\n",
        "4. move to the [**Bert-Synonyms/bertosaurus/bertosaurus**] folder of project\n",
        "5. here there are main working files.\n",
        "6. ***to run the program, use run.py file.***\n",
        "7. make sure you have **original sentances in text.txt file**, **nouns in nouns.txt file** and **output will be saved in the same directory with contextual-output.txt file.**\n",
        "\n"
      ],
      "metadata": {
        "id": "QS32RMAC-YTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cedricconol/bertosaurus.git"
      ],
      "metadata": {
        "id": "Ok-4gsohcN0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd bertosaurus/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqev-B25mOCX",
        "outputId": "dd62b95f-e464-469a-f755-48cbca322e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bertosaurus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BokpxT-9mk1O",
        "outputId": "eca9fce2-60b7-414f-c778-792e696fd969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all-nouns.txt  train.txt.raw               without-stopwords.txt\n",
            "\u001b[0m\u001b[01;34moutput_dir\u001b[0m/    unique-nouns.txt            wo-named-entities.txt\n",
            "\u001b[01;34msample_data\u001b[0m/   without-named-entities.txt  wo-stopwords.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "id": "VDwCd83kmldQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertosaurus import Bertosaurus\n"
      ],
      "metadata": {
        "id": "cFB37nAgmot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "OuLelsiXxMm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "PO3ro5kmxyZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_trf\")"
      ],
      "metadata": {
        "id": "6pey-BsQxq8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thesaurus = Bertosaurus()\n",
        "\n",
        "word = 'change'\n",
        "sentence = 'I want to change the world.' # sentence which uses the word\n",
        "\n",
        "thesaurus.ranked_synonyms(sentence, word)"
      ],
      "metadata": {
        "id": "I9bz8d0HnSm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*using* Genism \n",
        "it was not working well in our case.\n",
        "that's why we have avoided to use this.\n",
        "**Rather we focused on using SpaCy Pipeline for text pre-processing.**"
      ],
      "metadata": {
        "id": "SJ86Ruz0QMZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# text = open(\"dev.txt.raw\",\"rt\")\n",
        "# fout = open(\"without-stopwords.txt\", \"wt\")\n",
        "\n",
        "# for line in text:\n",
        "\n",
        "# #text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
        "#   filtered_sentence = remove_stopwords(line)\n",
        "#   fout.write(filtered_sentence + '\\n')\n",
        "\n",
        "#   print(filtered_sentence)"
      ],
      "metadata": {
        "id": "yljZxB42OHjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting both Common and Proper Nouns**"
      ],
      "metadata": {
        "id": "cl_TjYiCt0PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=open('without-stopwords.txt').read()\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "MA8-rvT76oc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'all-nouns.txt','w') as an:\n",
        "  for noun_chunk in doc.noun_chunks:\n",
        "    an.write(\"%s\\n\" % noun_chunk.text)\n",
        "    print(noun_chunk.text)"
      ],
      "metadata": {
        "id": "awnYcl84ofSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "printing noun's respective sentence in text"
      ],
      "metadata": {
        "id": "OSKN5J-yGNVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for noun_chunk in doc.noun_chunks:\n",
        "    print(noun_chunk.text, \"\\t\", noun_chunk.sent)"
      ],
      "metadata": {
        "id": "YcGWg2LYGRr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting Root of Nouns**"
      ],
      "metadata": {
        "id": "4l-AOTQPxllR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'root-nouns.txt','w') as rn:\n",
        "  for noun_chunk in doc.noun_chunks:\n",
        "    rn.write(\"%s\\n\" % noun_chunk.root.text)\n",
        "    print(noun_chunk.root.text)"
      ],
      "metadata": {
        "id": "G6sIpa6CxlN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting Proper Nouns from file**"
      ],
      "metadata": {
        "id": "P-A9F6IbqjCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tok in doc:\n",
        "    print(tok, tok.pos_)"
      ],
      "metadata": {
        "id": "a-o7LMP9qnLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_proper_nouns(doc):\n",
        "    pos = [tok.i for tok in doc if tok.pos_ == \"PROPN\"]\n",
        "    consecutives = []\n",
        "    current = []\n",
        "    for elt in pos:\n",
        "        if len(current) == 0:\n",
        "            current.append(elt)\n",
        "        else:\n",
        "            if current[-1] == elt - 1:\n",
        "                current.append(elt)\n",
        "            else:\n",
        "                consecutives.append(current)\n",
        "                current = [elt]\n",
        "    if len(current) != 0:\n",
        "        consecutives.append(current)\n",
        "    return [doc[consecutive[0]:consecutive[-1]+1] for consecutive in consecutives]"
      ],
      "metadata": {
        "id": "QSALPdyDqxG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**saving proper nouns in a file**"
      ],
      "metadata": {
        "id": "XzlFTCv0AL2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = extract_proper_nouns(doc)\n",
        "\n",
        "# open file in write mode\n",
        "with open(r'proper-nouns.txt.raw', 'w') as fp:\n",
        "    for item in names:\n",
        "        # write each item on a new line\n",
        "        fp.write(\"%s\\n\" % item)\n",
        "    print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra5YGIcM_zVI",
        "outputId": "03af608b-5179-4c49-f486-1204d732b209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**after finding unique-proper-nouns, named entities have to be filtered out.**"
      ],
      "metadata": {
        "id": "5UlnYiNwDdef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text_data = open('unique-proper-nouns.txt.raw').read()\n",
        "#text_data = 'New York is in USA'\n",
        "document = nlp(text_data)\n",
        "print(\" \".join([ent.text for ent in document if not ent.ent_type_]))"
      ],
      "metadata": {
        "id": "_4XFPSVIDdMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**finding synonyms of relavent words**"
      ],
      "metadata": {
        "id": "s8zva69PSjYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "kP9ETNTAT6KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "KkrS0PAhT9zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "synonyms = []\n",
        "\n",
        "for syn in wn.synsets(\"accident\"):\n",
        "    for i in syn.lemmas():\n",
        "        synonyms.append(i.name())\n",
        "\n",
        "print(set(synonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gZafgihSpFn",
        "outputId": "57eee88e-ec03-460c-dc7c-eeaff938d3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fortuity', 'chance_event', 'accident', 'stroke'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**using Word Embedding based approach to find similart contextual words **"
      ],
      "metadata": {
        "id": "NXN09SSVsghC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow>=2.0.0\"\n",
        "!pip install --upgrade tensorflow-hub"
      ],
      "metadata": {
        "id": "Jk0Cu-93snVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "  \n",
        "# Load pre trained ELMo model\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
        "  \n",
        "# create an instance of ELMo\n",
        "embeddings = elmo(\n",
        "    [\n",
        "        \"watch\",\n",
        "        \"school\"\n",
        "    ],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"elmo\"]\n",
        "init = tf.initialize_all_variables()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "  \n",
        "# Print word embeddings for word WATCH in given two sentences\n",
        "print('Word embeddings for word WATCH in first sentence')\n",
        "print(sess.run(embeddings[0][0]))\n",
        "print('Word embeddings for word WATCH in second sentence')\n",
        "print(sess.run(embeddings[1][0]))"
      ],
      "metadata": {
        "id": "iJ4QKl8ZsuUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('accident')\n",
        "\n",
        "text = nltk.Text(word.lower() for word in nltk.corpus.treebank.words())\n",
        "text.similar('accident')\n",
        "\n",
        "similar_words = text._word_context_index.similar_words('bucket')\n",
        "print(similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcBjWkND6t2l",
        "outputId": "d92a65fc-98ac-4847-fd6f-d66980808335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading accident: Package 'accident' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "issue executive official analyst economist ounce\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "\n",
        "\n",
        "for syn in wordnet.synsets(\"accident\"):\n",
        "\t\tfor l in syn.lemmas():\n",
        "\t\t\tsynonyms.append(l.name())\n",
        "\t\t\tif l.antonyms():\n",
        "\t\t\t\t antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(synonyms))\n",
        "#print(set(antonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAITvR3K7YPO",
        "outputId": "30f819f0-dc85-4eb7-b69d-07f8dae2997d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fortuity', 'chance_event', 'accident', 'stroke'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer=CountVectorizer()\n",
        "data_corpus=[\"guru99 is the best sitefor online tutorials. I love to visit guru99.\"]\n",
        "vocabulary=vectorizer.fit(data_corpus)\n",
        "X= vectorizer.transform(data_corpus)\n",
        "print(X.toarray())\n",
        "print(vocabulary.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clJcHVW-9Sq1",
        "outputId": "0cafb0d6-6fa0-4e57-a2fe-05886bc673d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 1 1 1 1 1 1 1 1]]\n",
            "['best', 'guru99', 'is', 'love', 'online', 'sitefor', 'the', 'to', 'tutorials', 'visit']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('abc')\n",
        "import gensim\n",
        "from nltk.corpus import abc\n",
        "\n",
        "model= gensim.models.Word2Vec(abc.sents())\n",
        "X= list(model.wv.vocab)\n",
        "data=model.most_similar('accident')\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOQOtXDC9lUv",
        "outputId": "0eac6c3d-62ff-409d-e3ca-cc51b107fe2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Package abc is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('sequencing', 0.9599843621253967), ('Cassini', 0.9547311067581177), ('art', 0.9539670944213867), ('trip', 0.9527146816253662), ('deposit', 0.9522777199745178), ('Learning', 0.952018141746521), ('origins', 0.951602578163147), ('epidemic', 0.9511515498161316), ('tyrannosaur', 0.9509668350219727), ('rally', 0.9508730173110962)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-184-29bb5b61741c>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  data=model.most_similar('accident')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Similar Sentances**"
      ],
      "metadata": {
        "id": "MHemV3OF0wB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install similar-sentences"
      ],
      "metadata": {
        "id": "Fp0P-VDx0zfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv dev.txt.raw dev.txt"
      ],
      "metadata": {
        "id": "Zxf3tgxA00Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051628a4-575c-446a-f7ce-956c0043620b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'dev.txt.raw': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SimilarSentences import SimilarSentences\n",
        "# Make sure the extension is .txt\n",
        "model = SimilarSentences('train.txt',\"train\")\n",
        "model.train()"
      ],
      "metadata": {
        "id": "UI4how690_s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from SimilarSentences import SimilarSentences\n",
        "model = SimilarSentences('/content/model.zip/',\"predict\")\n",
        "text = 'Tom is drinking water.'\n",
        "simple = model.predict(text, 2, \"simple\")\n",
        "detailed = model.predict(text, 2, \"detailed\")\n",
        "print(simple)\n",
        "print(detailed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VxypVVq1kT8",
        "outputId": "2e2ea40e-a5df-4602-de42-c1794f0b715a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For reloading/updating the model try model.relaod()\n",
            "[\"Tom was carrying a bucket of water.\", \"Tom lowered the bucket into the well.\"]\n",
            "[[{\"sentence\": \"Tom was carrying a bucket of water.\", \"score\": 0.7643241204994712, \"given\": \"Tom is drinking water.\"}], [{\"sentence\": \"Tom lowered the bucket into the well.\", \"score\": 0.6865937709606839, \"given\": \"Tom is drinking water.\"}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**creating smaller data files for execution**"
      ],
      "metadata": {
        "id": "3YHF9_O0HcYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir data-chunks1"
      ],
      "metadata": {
        "id": "x7KGwoNyHf_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_file = 'train.txt.raw'\n",
        "sorting = True\n",
        "hold_lines = []\n",
        "with open(my_file,'r') as text_file:\n",
        "    for row in text_file:\n",
        "        hold_lines.append(row)\n",
        "outer_count = 1\n",
        "line_count = 0\n",
        "while sorting:\n",
        "    count = 0\n",
        "    increment = (outer_count-1) * 20000\n",
        "    left = len(hold_lines) - increment\n",
        "    file_name = \"/content/data-chunks/train\" + str(outer_count * 20000) + \".txt.raw\"\n",
        "    hold_new_lines = []\n",
        "    if left < 20000:\n",
        "        while count < left:\n",
        "            hold_new_lines.append(hold_lines[line_count])\n",
        "            count += 1\n",
        "            line_count += 1\n",
        "        sorting = False\n",
        "    else:\n",
        "        while count < 20000:\n",
        "            hold_new_lines.append(hold_lines[line_count])\n",
        "            count += 1\n",
        "            line_count += 1\n",
        "    outer_count += 1\n",
        "    with open(file_name,'w') as next_file:\n",
        "        for row in hold_new_lines:\n",
        "            next_file.write(row)"
      ],
      "metadata": {
        "id": "MFDtvbCCHhcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}